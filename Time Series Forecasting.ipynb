{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bishrfaz/ClinicalBert/blob/master/Time%20Series%20Forecasting.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TIME SERIES DATA FORECASTING FAVOURITA DATASET**\n"
      ],
      "metadata": {
        "id": "DA2Az5PpkSX5"
      },
      "id": "DA2Az5PpkSX5"
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "qDNlZ7ehqodJ"
      },
      "id": "qDNlZ7ehqodJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importing necessary libraries\n"
      ],
      "metadata": {
        "id": "vwPFh9KDzH1o"
      },
      "id": "vwPFh9KDzH1o"
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Handling\n",
        "from dotenv import dotenv_values\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "\n",
        "# Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.cm as cm\n",
        "import matplotlib.patches as mpatches\n",
        "import seaborn as sns\n",
        "import plotly.express as px\n",
        "\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_squared_log_error\n",
        "\n",
        "# Feature Processing\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Modelling\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "from statsmodels.tsa.arima.model import ARIMA\n",
        "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
        "import xgboost as xgb\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "import pickle\n",
        "\n",
        "\n",
        "# Other Packages\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "YYbrNNs_zHWH"
      },
      "id": "YYbrNNs_zHWH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Getting Familarized with the Datasets"
      ],
      "metadata": {
        "id": "4HGqEcIYqJv8"
      },
      "id": "4HGqEcIYqJv8"
    },
    {
      "cell_type": "code",
      "source": [
        "#function to display the details of datasets\n",
        "def details(dataset,datasetname):\n",
        "  print(f\"dimension of {datasetname} {dataset.shape}\")\n",
        "  print(\"=\"*50)\n",
        "  print()\n",
        "  print(dataset.head())\n",
        "  print(\"=\"*50)\n",
        "  print()\n",
        "  print(dataset.info())\n",
        "  print(\"=\"*50)\n",
        "  print()\n",
        "  print(dataset.describe())\n",
        "  print(\"=\"*50)\n",
        "  print(\"Missing Values\")\n",
        "  print(dataset.isnull().sum())\n",
        "  print(\"=\"*50)\n",
        "  print()\n",
        "\n"
      ],
      "metadata": {
        "id": "TeZD40-O1Pi_"
      },
      "id": "TeZD40-O1Pi_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Detail of oil.csv"
      ],
      "metadata": {
        "id": "YXJDDHzA2NMA"
      },
      "id": "YXJDDHzA2NMA"
    },
    {
      "cell_type": "code",
      "source": [
        "#importing oil dataset from drive and displaying the details\n",
        "oil_df=pd.read_csv(\"/content/drive/MyDrive/data/oil.csv\")\n",
        "details(oil_df,\"oil.csv\")\n"
      ],
      "metadata": {
        "id": "elRvTJ1uy8Xy"
      },
      "id": "elRvTJ1uy8Xy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Details of holiday events.csv"
      ],
      "metadata": {
        "id": "aIZG-fy42SmV"
      },
      "id": "aIZG-fy42SmV"
    },
    {
      "cell_type": "code",
      "source": [
        "holiday_df=pd.read_csv(\"/content/drive/MyDrive/data/holidays_events.csv\")\n",
        "details(holiday_df,\"holiday_events.csv\")\n"
      ],
      "metadata": {
        "id": "ZMJERu7w2Xya"
      },
      "id": "ZMJERu7w2Xya",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Details of stores.csv\n"
      ],
      "metadata": {
        "id": "vw91qjZ42evu"
      },
      "id": "vw91qjZ42evu"
    },
    {
      "cell_type": "code",
      "source": [
        "stores_df=pd.read_csv(\"/content/drive/MyDrive/data/stores.csv\")\n",
        "details(stores_df,\"stores.csv\")\n"
      ],
      "metadata": {
        "id": "p-5rQL-p2mUX"
      },
      "id": "p-5rQL-p2mUX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Details of transction.csv\n"
      ],
      "metadata": {
        "id": "-LTcf_PS2uJB"
      },
      "id": "-LTcf_PS2uJB"
    },
    {
      "cell_type": "code",
      "source": [
        "transactions_df=pd.read_csv(\"/content/drive/MyDrive/data/transactions.csv\")\n",
        "details(transactions_df,\"transactions.csv\")\n"
      ],
      "metadata": {
        "id": "XUwOIAFo2tte"
      },
      "id": "XUwOIAFo2tte",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Details of train.csv"
      ],
      "metadata": {
        "id": "rudBgWKU250B"
      },
      "id": "rudBgWKU250B"
    },
    {
      "cell_type": "code",
      "source": [
        "train_df=pd.read_csv(\"/content/drive/MyDrive/data/train.csv\")\n",
        "details(train_df,\"train.csv\")\n"
      ],
      "metadata": {
        "id": "4I5QVS7-2-a6"
      },
      "id": "4I5QVS7-2-a6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Details of the test.csv"
      ],
      "metadata": {
        "id": "3Casj9wi32rS"
      },
      "id": "3Casj9wi32rS"
    },
    {
      "cell_type": "code",
      "source": [
        "test_df=pd.read_csv(\"/content/drive/MyDrive/data/test.csv\")\n",
        "details(test_df,\"test.csv\")\n"
      ],
      "metadata": {
        "id": "gcUukGBJ36Ez"
      },
      "id": "gcUukGBJ36Ez",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YhOayv_X8qg5"
      },
      "id": "YhOayv_X8qg5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "By looking upon the details we came to know that the datatype of date is object for exploitation the datatype should be changed to datetime\n",
        "\n",
        "\n",
        "There are 43 missing values in oil price in oil.csv"
      ],
      "metadata": {
        "id": "EWfgLQkw6ZUC"
      },
      "id": "EWfgLQkw6ZUC"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transforming Object to Datetime"
      ],
      "metadata": {
        "id": "4V-G5TBv8xuM"
      },
      "id": "4V-G5TBv8xuM"
    },
    {
      "cell_type": "code",
      "source": [
        "def datetimetransform(dataset):\n",
        "  dataset[\"date\"]=pd.to_datetime(dataset[\"date\"])\n",
        "datetimetransform(oil_df)\n",
        "datetimetransform(holiday_df)\n",
        "datetimetransform(transactions_df)\n",
        "datetimetransform(train_df)\n",
        "datetimetransform(test_df)"
      ],
      "metadata": {
        "id": "JFfJ4dVX84QU"
      },
      "id": "JFfJ4dVX84QU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Handling missing values"
      ],
      "metadata": {
        "id": "R4HtkHDw9tQF"
      },
      "id": "R4HtkHDw9tQF"
    },
    {
      "cell_type": "code",
      "source": [
        "#visualize the oil.csv to see how large the gaps are\n",
        "fig=px.line(oil_df,x='date',y=\"dcoilwtico\")\n",
        "fig.update_layout(title=\"Trend of oil prices\",xaxis_title=\"Date\",yaxis_title='oil price')\n",
        "fig.update_xaxes(rangeslider_visible=True)\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "mKyFh2Ll9yBq"
      },
      "id": "mKyFh2Ll9yBq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Backward fill is suitable as there are irregular gaps in  the plot an its the most suitable way as the future values are the best estimator for filling the gap"
      ],
      "metadata": {
        "id": "5cN7qQaaNw3m"
      },
      "id": "5cN7qQaaNw3m"
    },
    {
      "cell_type": "code",
      "source": [
        "oil_df['dcoilwtico']=oil_df['dcoilwtico'].fillna(method='bfill')"
      ],
      "metadata": {
        "id": "bfoUY_R1OMpW"
      },
      "id": "bfoUY_R1OMpW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cross checking for missing values in oil.csv"
      ],
      "metadata": {
        "id": "7pIOrmJ9OqAc"
      },
      "id": "7pIOrmJ9OqAc"
    },
    {
      "cell_type": "code",
      "source": [
        "details(oil_df,\"oil.csv\")"
      ],
      "metadata": {
        "id": "6fHOhPE9Ox3Q"
      },
      "id": "6fHOhPE9Ox3Q",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check the continouity of the Date in the test.csv"
      ],
      "metadata": {
        "id": "RfL0m-UmPOO2"
      },
      "id": "RfL0m-UmPOO2"
    },
    {
      "cell_type": "code",
      "source": [
        "min_date = train_df['date'].min()\n",
        "max_date = train_df['date'].max()\n",
        "expected_dates = pd.date_range(start=min_date, end=max_date)\n",
        "\n",
        "missing_dates = expected_dates[~expected_dates.isin(train_df['date'])]\n",
        "\n",
        "if len(missing_dates) == 0:\n",
        "    print(\"The train dataset is complete. It includes all the required dates.\")\n",
        "else:\n",
        "    print(\"The train dataset is incomplete. The following dates are missing:\")\n",
        "    print(missing_dates)\n"
      ],
      "metadata": {
        "id": "6AseCpgLPZ2C"
      },
      "id": "6AseCpgLPZ2C",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "handle the missing dates"
      ],
      "metadata": {
        "id": "rQEGxAtoQzCM"
      },
      "id": "rQEGxAtoQzCM"
    },
    {
      "cell_type": "code",
      "source": [
        "# Complete the missing dates in the train dataset\n",
        "# Create an index of the missing dates as a DatetimeIndex object\n",
        "missing_dates = pd.Index(['2013-12-25', '2014-12-25', '2015-12-25', '2016-12-25'], dtype='datetime64[ns]')\n",
        "\n",
        "# Create a DataFrame with the missing dates, using the 'date' column\n",
        "missing_data = pd.DataFrame({'date': missing_dates})\n",
        "\n",
        "# Concatenate the original train dataset and the missing data DataFrame\n",
        "# ignore_index=True ensures a new index is assigned to the resulting DataFrame\n",
        "train_df = pd.concat([train_df, missing_data], ignore_index=True)\n",
        "\n",
        "# Sort the DataFrame based on the 'date' column in ascending order\n",
        "train_df.sort_values('date', inplace=True)"
      ],
      "metadata": {
        "id": "Dp5vn2yPR4BT"
      },
      "id": "Dp5vn2yPR4BT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "confirming the dataset\n"
      ],
      "metadata": {
        "id": "CQr1eP19R-fP"
      },
      "id": "CQr1eP19R-fP"
    },
    {
      "cell_type": "code",
      "source": [
        "min_date = train_df['date'].min()\n",
        "max_date = train_df['date'].max()\n",
        "expected_dates = pd.date_range(start=min_date, end=max_date)\n",
        "\n",
        "missing_dates = expected_dates[~expected_dates.isin(train_df['date'])]\n",
        "\n",
        "if len(missing_dates) == 0:\n",
        "    print(\"The train dataset is complete. It includes all the required dates.\")\n",
        "else:\n",
        "    print(\"The train dataset is incomplete. The following dates are missing:\")\n",
        "    print(missing_dates)\n"
      ],
      "metadata": {
        "id": "6xz1aHUzSCLb"
      },
      "id": "6xz1aHUzSCLb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Merge the oil.csv,holiday_events.csv,tranaction.csv to the train.csv using inner join on date coloumn\n",
        "\n",
        "merge store.csv using store_nbr column"
      ],
      "metadata": {
        "id": "toQ4HcnvSJSq"
      },
      "id": "toQ4HcnvSJSq"
    },
    {
      "cell_type": "code",
      "source": [
        "# Merging the common columns ('store_nbr' and 'date') in the datasets using the inner merge() function\n",
        "# Merge train_df with stores_df based on 'store_nbr' column\n",
        "merged_df1 = train_df.merge(stores_df, on='store_nbr', how='inner')\n",
        "\n",
        "# Merge merged_df1 with transactions_df based on 'date' and 'store_nbr' columns\n",
        "merged_df2 = merged_df1.merge(transactions_df, on=['date', 'store_nbr'], how='inner')\n",
        "\n",
        "# Merge merged_df2 with holidays_events_df based on 'date' column\n",
        "merged_df3 = merged_df2.merge(holiday_df, on='date', how='inner')\n",
        "\n",
        "# Merge merged_df3 with oil_df based on 'date' column\n",
        "merged_df = merged_df3.merge(oil_df, on='date', how='inner')\n",
        "\n",
        "# View the first five rows of the merged dataset\n",
        "merged_df.head()"
      ],
      "metadata": {
        "id": "lsbZDqC9S0Il"
      },
      "id": "lsbZDqC9S0Il",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The use of an inner merge in this time series forecasting project for corporation Favorita helps to ensure data consistency, avoid missing values, and focus on the relevant data for accurate predictions.\n",
        "\n",
        "An inner merge type retains only the rows with matching values in the specified columns. In the context of time series forecasting, it allows us to merge datasets based on a common time index or timestamp. By performing an inner merge, we ensure that only the rows with corresponding timestamps in both datasets are included in the merged result. This is important for time series forecasting because you want to align the data from different sources based on their timestamps to build a consistent and accurate forecasting model.\n",
        "\n",
        "With an inner merge, you eliminate any non-matching timestamps, which may not be useful for forecasting and could introduce inconsistencies in the data. By focusing on the intersection of the datasets, we can create a merged dataset that contains the necessary information for accurate time series forecasting."
      ],
      "metadata": {
        "id": "aJeLOQMjTb1D"
      },
      "id": "aJeLOQMjTb1D"
    },
    {
      "cell_type": "markdown",
      "source": [
        "checking the details of merged dataset\n"
      ],
      "metadata": {
        "id": "phEYLnRKTGvs"
      },
      "id": "phEYLnRKTGvs"
    },
    {
      "cell_type": "code",
      "source": [
        "details(merged_df,\"merged_df\")\n"
      ],
      "metadata": {
        "id": "LUAFQA6KTMzU"
      },
      "id": "LUAFQA6KTMzU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The merged dataset after merging the train dataset with additional datasets contains 322,047 rows and 16 columns. Two columns have been renamed as a result of the merging, type_x and type_y."
      ],
      "metadata": {
        "id": "SwDwDPYzTt4M"
      },
      "id": "SwDwDPYzTt4M"
    },
    {
      "cell_type": "markdown",
      "source": [
        "checking for the details of the newly created columns\n"
      ],
      "metadata": {
        "id": "hveOwOymT2X9"
      },
      "id": "hveOwOymT2X9"
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the unique values of the two unknown columns\n",
        "print(\"Unique values of 'type_x':\")\n",
        "print(merged_df['type_x'].unique())\n",
        "print()\n",
        "print(\"Unique values of 'type_y':\")\n",
        "print(merged_df['type_y'].unique())"
      ],
      "metadata": {
        "id": "Pjq8WVAtT7Ot"
      },
      "id": "Pjq8WVAtT7Ot",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Unique values of 'type_x':\n",
        "['D' 'E' 'C' 'A' 'B']\n",
        "\n",
        "Unique values of 'type_y':\n",
        "\n",
        "['Holiday' 'Additional' 'Transfer' 'Event' 'Bridge']\n",
        "\n",
        "The merged dataset consists of 322,047 non-null observations. Two columns have been renamed as a result of the merging, type_x and type_y.\n",
        "\n",
        "The type_x column represents the store type.\n",
        "\n",
        "The type_y column represents the holiday type."
      ],
      "metadata": {
        "id": "Wc39v0lEUWJ1"
      },
      "id": "Wc39v0lEUWJ1"
    },
    {
      "cell_type": "code",
      "source": [
        "merged_df = merged_df.rename(columns={\"type_x\": \"store_type\", \"type_y\": \"holiday_type\"})\n",
        "merged_df.head()"
      ],
      "metadata": {
        "id": "50DI_KBXUVLe"
      },
      "id": "50DI_KBXUVLe",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "checking details of merged dataset\n"
      ],
      "metadata": {
        "id": "hJrNL71NUmFA"
      },
      "id": "hJrNL71NUmFA"
    },
    {
      "cell_type": "code",
      "source": [
        "details(merged_df,\"merged_df\")"
      ],
      "metadata": {
        "id": "rOP9QPvMUqhc"
      },
      "id": "rOP9QPvMUqhc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**SAVING THE MERGED DATASET**"
      ],
      "metadata": {
        "id": "gL9Lwn1hU00e"
      },
      "id": "gL9Lwn1hU00e"
    },
    {
      "cell_type": "code",
      "source": [
        "merged_df.to_csv('/content/drive/MyDrive/data/Visualization_Data.csv', index=False)\n",
        "merged_df.head()"
      ],
      "metadata": {
        "id": "WIuA2Sl7U6yL"
      },
      "id": "WIuA2Sl7U6yL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Feature Engineering**\n"
      ],
      "metadata": {
        "id": "ZqjhPml5movO"
      },
      "id": "ZqjhPml5movO"
    },
    {
      "cell_type": "markdown",
      "source": [
        "create a copy of the merged dataset"
      ],
      "metadata": {
        "id": "M0z2IvHTmwur"
      },
      "id": "M0z2IvHTmwur"
    },
    {
      "cell_type": "code",
      "source": [
        "mergeddf_copy=merged_df.copy()\n",
        "mergeddf_copy.head()\n",
        "details(mergeddf_copy,'merged_df')"
      ],
      "metadata": {
        "id": "urzL3DBJozC2"
      },
      "id": "urzL3DBJozC2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extracting date components for getting sesonal, yearly and weekly trends"
      ],
      "metadata": {
        "id": "_8semNYgo8yR"
      },
      "id": "_8semNYgo8yR"
    },
    {
      "cell_type": "code",
      "source": [
        "mergeddf_copy['year'] = mergeddf_copy['date'].dt.year\n",
        "mergeddf_copy['month'] = mergeddf_copy['date'].dt.month\n",
        "mergeddf_copy['day'] = mergeddf_copy['date'].dt.day\n",
        "mergeddf_copy.head()"
      ],
      "metadata": {
        "id": "1uhXfYX0ppcq"
      },
      "id": "1uhXfYX0ppcq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Uh1wXduRsCjW"
      },
      "id": "Uh1wXduRsCjW"
    },
    {
      "cell_type": "markdown",
      "source": [
        "droping columns which are not required for forecasting"
      ],
      "metadata": {
        "id": "kkLS4D2psJv5"
      },
      "id": "kkLS4D2psJv5"
    },
    {
      "cell_type": "code",
      "source": [
        "columns_to_drop = ['date','id', 'locale', 'locale_name', 'description', 'store_type', 'transferred', 'state']\n",
        "mergeddf_copy = mergeddf_copy.drop(columns=columns_to_drop)\n",
        "\n",
        "mergeddf_copy.head()"
      ],
      "metadata": {
        "id": "n1TwBhf8r-Z2"
      },
      "id": "n1TwBhf8r-Z2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Classification of items to different product families"
      ],
      "metadata": {
        "id": "O6ILSimZsUO_"
      },
      "id": "O6ILSimZsUO_"
    },
    {
      "cell_type": "code",
      "source": [
        "unique_families = mergeddf_copy['family'].unique()\n",
        "unique_families"
      ],
      "metadata": {
        "id": "rsdx1Pr6sfRf"
      },
      "id": "rsdx1Pr6sfRf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "food_families = ['BEVERAGES', 'BREAD/BAKERY', 'FROZEN FOODS', 'MEATS', 'PREPARED FOODS', 'DELI','PRODUCE', 'DAIRY','POULTRY','EGGS','SEAFOOD']\n",
        "home_families = ['HOME AND KITCHEN I', 'HOME AND KITCHEN II', 'HOME APPLIANCES']\n",
        "clothing_families = ['LINGERIE', 'LADIESWEAR']\n",
        "grocery_families = ['GROCERY I', 'GROCERY II']\n",
        "stationery_families = ['BOOKS', 'MAGAZINES','SCHOOL AND OFFICE SUPPLIES']\n",
        "cleaning_families = ['HOME CARE', 'BABY CARE','PERSONAL CARE']\n",
        "hardware_families = ['PLAYERS AND ELECTRONICS','HARDWARE']\n",
        "\n",
        "\n",
        "mergeddf_copy['family'] = np.where(mergeddf_copy['family'].isin(food_families), 'FOODS', mergeddf_copy['family'])\n",
        "mergeddf_copy['family'] = np.where(mergeddf_copy['family'].isin(home_families), 'HOME', mergeddf_copy['family'])\n",
        "mergeddf_copy['family'] = np.where(mergeddf_copy['family'].isin(clothing_families), 'CLOTHING', mergeddf_copy['family'])\n",
        "mergeddf_copy['family'] = np.where(mergeddf_copy['family'].isin(grocery_families), 'GROCERY', mergeddf_copy['family'])\n",
        "mergeddf_copy['family'] = np.where(mergeddf_copy['family'].isin(stationery_families), 'STATIONERY', mergeddf_copy['family'])\n",
        "mergeddf_copy['family'] = np.where(mergeddf_copy['family'].isin(cleaning_families), 'CLEANING', mergeddf_copy['family'])\n",
        "mergeddf_copy['family'] = np.where(mergeddf_copy['family'].isin(hardware_families), 'HARDWARE', mergeddf_copy['family'])\n",
        "\n",
        "\n",
        "mergeddf_copy.head()"
      ],
      "metadata": {
        "id": "5kBzJgDIspn_"
      },
      "id": "5kBzJgDIspn_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "glww3hhYbt6r"
      },
      "id": "glww3hhYbt6r",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Histogram of Sales\n",
        "plt.figure(figsize=(10,6))\n",
        "plt.hist(merged_df['sales'], bins=20, color=\"steelblue\", edgecolor=\"black\")\n",
        "plt.xlabel('Sales')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Distribution of Sales')\n",
        "plt.show()\n",
        "\n",
        "# Boxplot of Sales\n",
        "plt.figure(figsize=(10,4))\n",
        "plt.boxplot(merged_df['sales'])\n",
        "plt.ylabel('Sales')\n",
        "plt.title('Boxplot of Sales')\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "gwEvtmRgk9nY"
      },
      "execution_count": null,
      "outputs": [],
      "id": "gwEvtmRgk9nY"
    },
    {
      "cell_type": "markdown",
      "source": [
        "distribution of the 'transactions' variable:"
      ],
      "metadata": {
        "id": "5W7yz45ns9Tc"
      },
      "id": "5W7yz45ns9Tc"
    },
    {
      "cell_type": "code",
      "source": [
        "# Histogram of Transactions\n",
        "plt.figure(figsize=(10,6))\n",
        "plt.hist(merged_df['transactions'], bins=20, color=\"steelblue\", edgecolor=\"black\")\n",
        "plt.xlabel('Transactions')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Distribution of Transactions')\n",
        "plt.show()\n",
        "\n",
        "# Boxplot of Transactions\n",
        "plt.figure(figsize=(10,4))\n",
        "plt.boxplot(merged_df['transactions'])\n",
        "plt.ylabel('Transactions')\n",
        "plt.title('Boxplot of Transactions')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Njfo9hIvtA6e"
      },
      "execution_count": null,
      "outputs": [],
      "id": "Njfo9hIvtA6e"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Distribution of the 'Daily Oil Price' variable:"
      ],
      "metadata": {
        "id": "-xYv9dCyuH8D"
      },
      "id": "-xYv9dCyuH8D"
    },
    {
      "cell_type": "code",
      "source": [
        "# Histogram of Oil Price\n",
        "plt.figure(figsize=(10,6))\n",
        "plt.hist(merged_df['dcoilwtico'].dropna(), bins=20, color=\"teal\", edgecolor=\"black\")\n",
        "plt.xlabel('Oil Price')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Distribution of Oil Price')\n",
        "plt.show()\n",
        "\n",
        "# Boxplot of Oil Price\n",
        "plt.figure(figsize=(10,4))\n",
        "plt.boxplot(merged_df['dcoilwtico'].dropna())\n",
        "plt.ylabel('Oil Price')\n",
        "plt.title('Boxplot of Oil Price')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "YPSO21vauKgO"
      },
      "execution_count": null,
      "outputs": [],
      "id": "YPSO21vauKgO"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Trend of sales over time."
      ],
      "metadata": {
        "id": "PRYeV9L_u26N"
      },
      "id": "PRYeV9L_u26N"
    },
    {
      "cell_type": "code",
      "source": [
        "# Group the data by date and calculate the total sales\n",
        "daily_sales = merged_df.groupby('date')['sales'].sum().reset_index()\n",
        "\n",
        "# Create a time series plot with slider\n",
        "fig = px.line(daily_sales, x='date', y='sales')\n",
        "fig.update_xaxes(rangeslider_visible=True)\n",
        "fig.update_layout(title='Trend of Sales Over Time', title_x=0.5)\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "Dfim2U9du0Dg"
      },
      "execution_count": null,
      "outputs": [],
      "id": "Dfim2U9du0Dg"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Trend of Daily Crude oil Prices Over Time"
      ],
      "metadata": {
        "id": "pKqzWf_Nu_22"
      },
      "id": "pKqzWf_Nu_22"
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the 'dcoilwtico' column to confirm if the trend is consistent.\n",
        "fig = px.line(oil_df, x='date', y='dcoilwtico')\n",
        "fig.update_layout(title='Trend of Oil Prices Over Time', title_x=0.5, xaxis_title='Date', yaxis_title='Oil Price')\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "5ClC0w5nvAsA"
      },
      "execution_count": null,
      "outputs": [],
      "id": "5ClC0w5nvAsA"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Total Count of Sales by Store Type"
      ],
      "metadata": {
        "id": "6E_eooYevIfg"
      },
      "id": "6E_eooYevIfg"
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the color palette to \"viridis\"\n",
        "sns.set_palette(\"viridis\")\n",
        "# Calculate the total count and total sales per store type\n",
        "store_type_counts = merged_df['store_type'].value_counts()\n",
        "store_type_sales = merged_df.groupby('store_type')['sales'].sum()\n",
        "\n",
        "# Create a bar plot with \"viridis\" color palette for total count\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.barplot(x=store_type_counts.index, y=store_type_counts.values)\n",
        "plt.xlabel('Store Type')\n",
        "plt.ylabel('Count')\n",
        "plt.title('Total Count by Store Type')\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "MBiIWFunvKMW"
      },
      "execution_count": null,
      "outputs": [],
      "id": "MBiIWFunvKMW"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Total Amount in Sales by Store Type"
      ],
      "metadata": {
        "id": "KWVQyOgav4qE"
      },
      "id": "KWVQyOgav4qE"
    },
    {
      "cell_type": "code",
      "source": [
        "# Order the store types by total sales\n",
        "store_type_sales = store_type_sales.sort_values(ascending=False)\n",
        "\n",
        "# Create a bar plot with \"viridis\" color palette for total sales\n",
        "plt.figure(figsize=(8, 8))\n",
        "sns.barplot(x=store_type_sales.index, y=store_type_sales.values, order=store_type_sales.index, palette=\"viridis\")\n",
        "plt.xlabel('Store Type')\n",
        "plt.ylabel('Total Sales')\n",
        "plt.title('Total Sales by Store Type')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "-AdlVILsvskx"
      },
      "execution_count": null,
      "outputs": [],
      "id": "-AdlVILsvskx"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Average Sales by City"
      ],
      "metadata": {
        "id": "X0SbCsEGv5o2"
      },
      "id": "X0SbCsEGv5o2"
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.cm as cm\n",
        "\n",
        "# Group by city and calculate the average sales\n",
        "average_sales_by_city = merged_df.groupby('city')['sales'].mean()\n",
        "\n",
        "# Sort the data by average sales in ascending order\n",
        "average_sales_by_city = average_sales_by_city.sort_values(ascending=True)\n",
        "\n",
        "# Define colors for the bar plot using 'viridis' color palette\n",
        "colors = cm.viridis(np.linspace(0, 1, len(average_sales_by_city)))\n",
        "\n",
        "# Plot the average sales by city horizontally\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.barh(average_sales_by_city.index, average_sales_by_city.values, color=colors)\n",
        "plt.xlabel('Average Sales')\n",
        "plt.ylabel('City')\n",
        "plt.title('Average Sales by City')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "jnidjxbRv96u"
      },
      "execution_count": null,
      "outputs": [],
      "id": "jnidjxbRv96u"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Average Sales by State"
      ],
      "metadata": {
        "id": "DZIO4t5nwURH"
      },
      "id": "DZIO4t5nwURH"
    },
    {
      "cell_type": "code",
      "source": [
        "# Group by state and calculate the average sales\n",
        "average_sales_by_state = merged_df.groupby('state')['sales'].mean()\n",
        "\n",
        "# Sort the data by average sales in descending order\n",
        "average_sales_by_state = average_sales_by_state.sort_values(ascending=True)\n",
        "\n",
        "# Plot the average sales by state\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.barh(average_sales_by_state.index, average_sales_by_state.values, color=colors)\n",
        "plt.xlabel('Average Sales')\n",
        "plt.ylabel('State')\n",
        "plt.title('Average Sales by State')\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Vew5KnlIwNgS"
      },
      "execution_count": null,
      "outputs": [],
      "id": "Vew5KnlIwNgS"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Relationship between sales and transactions."
      ],
      "metadata": {
        "id": "hz-2jzMzwZ57"
      },
      "id": "hz-2jzMzwZ57"
    },
    {
      "cell_type": "code",
      "source": [
        "sns.scatterplot(x='transactions', y='sales', data=merged_df)\n",
        "plt.xlabel('Transactions')\n",
        "plt.ylabel('Sales')\n",
        "plt.title('Relationship between Sales and Transactions')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "rFH0DME2wZkX"
      },
      "execution_count": null,
      "outputs": [],
      "id": "rFH0DME2wZkX"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Correlation matrix of numerical variables"
      ],
      "metadata": {
        "id": "K-visHUtwjdT"
      },
      "id": "K-visHUtwjdT"
    },
    {
      "cell_type": "code",
      "source": [
        "# Select numerical variables for correlation analysis\n",
        "numerical_vars = ['sales', 'transactions', 'dcoilwtico']\n",
        "\n",
        "# Compute correlation matrix\n",
        "corr_matrix = merged_df[numerical_vars].corr()\n",
        "\n",
        "# Plot heatmap\n",
        "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm')\n",
        "plt.title('Correlation Matrix')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "L9JaMk46wkRS"
      },
      "execution_count": null,
      "outputs": [],
      "id": "L9JaMk46wkRS"
    },
    {
      "cell_type": "markdown",
      "source": [
        "b. Scatter Plot Marrix of numerical Variables\n"
      ],
      "metadata": {
        "id": "nXNeiZXpwri_"
      },
      "id": "nXNeiZXpwri_"
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import cm\n",
        "# Select numerical variables for correlation analysis\n",
        "numerical_vars = ['sales', 'transactions', 'dcoilwtico']\n",
        "\n",
        "# Plot scatter plot matrix\n",
        "sns.pairplot(merged_df[numerical_vars])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "N-s_SCNaE2nb"
      },
      "execution_count": null,
      "outputs": [],
      "id": "N-s_SCNaE2nb"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stationarity Test\n",
        "\n",
        "Stationarity implies that the statistical properties of the time series, such as mean and variance, remain constant over time. In this case, the ADF test was conducted on the 'sales' data from the 'merged_df' dataset. To perform the stationarity test, we will use the Augmented Dickey-Fuller (ADF) test commonly used to check for stationarity in a time series.\n",
        "\n",
        "Null hypothesis (H0): The sales data is non-stationary.\n",
        "Alternative hypothesis (H1): The sales data is stationary."
      ],
      "metadata": {
        "id": "gMgpd50exkaA"
      },
      "id": "gMgpd50exkaA"
    },
    {
      "cell_type": "code",
      "source": [
        "from statsmodels.tsa.stattools import adfuller\n",
        "# Statistical Test of the 'sales' column in the merged_df using Adfuller\n",
        "sales_data = merged_df['sales']\n",
        "\n",
        "# Perform ADF test\n",
        "result = adfuller(sales_data)\n",
        "\n",
        "# Extract the test statistics and p-value from the result\n",
        "test_statistic = result[0]\n",
        "p_value = result[1]\n",
        "critical_values = result[4]\n",
        "\n",
        "# Print the test statistics and critical values\n",
        "print(f\"ADF Test Statistics: {test_statistic}\")\n",
        "print(f\"P-value: {p_value}\")\n",
        "print(\"Critical Values:\")\n",
        "for key, value in critical_values.items():\n",
        "    print(f\"   {key}: {value}\")\n",
        "\n",
        "# Check the p-value against a significance level (e.g., 0.05)\n",
        "if p_value <= 0.05:\n",
        "    print(\"Reject the null hypothesis: The sales data is stationary.\")\n",
        "else:\n",
        "    print(\"Fail to reject the null hypothesis: The sales data is non-stationary.\")"
      ],
      "metadata": {
        "id": "bz9VslaRxrV6"
      },
      "execution_count": null,
      "outputs": [],
      "id": "bz9VslaRxrV6"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hypothesis Testing and Answering Key Analytical Questions\n",
        "\n",
        "Hypothesis Testing\n",
        "Null Hypothesis (H0): The promotional activities have a significant impact on store sales for Corporation Favorita.\n",
        "\n",
        "Alternative Hypothesis (H1): The promotional activities have a significant impact on store sales for Corporation Favorita."
      ],
      "metadata": {
        "id": "WpLUEw9ox-EJ"
      },
      "id": "WpLUEw9ox-EJ"
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract the relevant variables for the hypothesis test\n",
        "promo_sales = merged_df[merged_df['onpromotion'] == 1]['sales']\n",
        "non_promo_sales = merged_df[merged_df['onpromotion'] == 0]['sales']\n",
        "\n",
        "# Perform a two-sample t-test to compare sales between promotional and non-promotional periods\n",
        "t_statistic, p_value = ttest_ind(promo_sales, non_promo_sales)\n",
        "\n",
        "\n",
        "from scipy.stats import ttest_ind\n",
        "\n",
        "# Extract the relevant variables for the hypothesis test\n",
        "promo_sales = merged_df[merged_df['onpromotion'] == 1]['sales']\n",
        "non_promo_sales = merged_df[merged_df['onpromotion'] == 0]['sales']\n",
        "\n",
        "# Perform a two-sample t-test to compare sales between promotional and non-promotional periods\n",
        "t_statistic, p_value = ttest_ind(promo_sales, non_promo_sales)\n",
        "\n",
        "# Print the test result\n",
        "print(\"Hypothesis Testing for Promotional Activities:\")\n",
        "print(\"Null Hypothesis (H0): The promotional activities do not have a significant impact on store sales.\")\n",
        "print(\"Alternative Hypothesis (H1): The promotional activities have a significant impact on store sales.\")\n",
        "print(\"-\" * 50)\n",
        "print(\"Test Statistic:\", t_statistic)\n",
        "print(\"P-value:\", p_value)\n",
        "print(\"=\" * 50)\n",
        "if p_value < 0.05:\n",
        "    print(\"Reject the null hypothesis. Promotional activities have a significant impact on store sales at Corporation Favorita.\")\n",
        "else:\n",
        "    print(\"Fail to reject the null hypothesis. Promotional activities do not have a significant impact on store sales at Corporation Favorita.\")"
      ],
      "metadata": {
        "id": "M7_sMeZnyFut"
      },
      "execution_count": null,
      "outputs": [],
      "id": "M7_sMeZnyFut"
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Is the train dataset complete (has all the required dates)?"
      ],
      "metadata": {
        "id": "9TVxkQgeyUtS"
      },
      "id": "9TVxkQgeyUtS"
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the completeness of the train dataset\n",
        "min_date = train_df['date'].min()\n",
        "max_date = train_df['date'].max()\n",
        "expected_dates = pd.date_range(start=min_date, end=max_date)\n",
        "\n",
        "missing_dates = expected_dates[~expected_dates.isin(train_df['date'])]\n",
        "\n",
        "if len(missing_dates) == 0:\n",
        "    print(\"The train dataset is complete. It includes all the required dates.\")\n",
        "else:\n",
        "    print(\"The train dataset is incomplete. The following dates are missing:\")\n",
        "    print(missing_dates)"
      ],
      "metadata": {
        "id": "XXTR4plcyXaD"
      },
      "execution_count": null,
      "outputs": [],
      "id": "XXTR4plcyXaD"
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Which dates have the lowest and highest sales for each year?"
      ],
      "metadata": {
        "id": "VBtoAm4WyanE"
      },
      "id": "VBtoAm4WyanE"
    },
    {
      "cell_type": "code",
      "source": [
        "merged_df['date'] = pd.to_datetime(merged_df['date'])\n",
        "merged_df['year'] = merged_df['date'].dt.year\n",
        "\n",
        "lowest_sales_dates = merged_df.groupby('year')['date'].min()\n",
        "highest_sales_dates = merged_df.groupby('year')['date'].max()\n",
        "\n",
        "print(\"Dates with the lowest sales for each year:\\n\", lowest_sales_dates)\n",
        "print(\"=\"*50)\n",
        "print(\"Dates with the highest sales for each year:\\n\", highest_sales_dates)"
      ],
      "metadata": {
        "id": "DcQG0D2nydmV"
      },
      "execution_count": null,
      "outputs": [],
      "id": "DcQG0D2nydmV"
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Analyze the impact of the earthquake on sales"
      ],
      "metadata": {
        "id": "RFemvkGCygdL"
      },
      "id": "RFemvkGCygdL"
    },
    {
      "cell_type": "code",
      "source": [
        "# Assign the variable earthquake_date to the date the earthquake took place (April 16, 2016)\n",
        "earthquake_date = pd.to_datetime('2016-04-16')\n",
        "\n",
        "# Filter the sales data before and after the earthquake\n",
        "sales_before_earthquake = train_df[train_df['date'] < earthquake_date]['sales']\n",
        "sales_after_earthquake = train_df[train_df['date'] > earthquake_date]['sales']\n",
        "\n",
        "# Set the colormap to viridis\n",
        "colormap = cm.get_cmap('viridis')\n",
        "\n",
        "# Plot the sales before and after the earthquake\n",
        "plt.plot(sales_before_earthquake, color=colormap(0.2), label='Sales Before Earthquake')\n",
        "plt.plot(sales_after_earthquake, color=colormap(0.8), label='Sales After Earthquake')\n",
        "plt.xlabel('Time')\n",
        "plt.ylabel('Sales')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "GyS5uGWAyi0U"
      },
      "execution_count": null,
      "outputs": [],
      "id": "GyS5uGWAyi0U"
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Determine if certain groups of stores sell more products"
      ],
      "metadata": {
        "id": "KFD-rsghylXi"
      },
      "id": "KFD-rsghylXi"
    },
    {
      "cell_type": "code",
      "source": [
        "# Group by cluster and calculate the average sales\n",
        "average_sales_by_cluster = merged_df.groupby('cluster')['sales'].mean()\n",
        "\n",
        "# Group by city and calculate the average sales\n",
        "average_sales_by_city = merged_df.groupby('city')['sales'].mean()\n",
        "\n",
        "# Group by state and calculate the average sales\n",
        "average_sales_by_state = merged_df.groupby('state')['sales'].mean()\n",
        "\n",
        "# Group by store type and calculate the average sales\n",
        "average_sales_by_store_type = merged_df.groupby('store_type')['sales'].mean()\n",
        "\n",
        "# Set the number of bars in each plot\n",
        "num_bars = len(average_sales_by_cluster)\n",
        "\n",
        "# Generate the colors using the viridis palette\n",
        "colors = plt.cm.viridis(np.linspace(0, 1, num_bars))\n",
        "# Sort the data by average sales in descending order\n",
        "average_sales_by_cluster = average_sales_by_cluster.sort_values(ascending=False)\n",
        "\n",
        "# Plot the average sales by cluster\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.bar(average_sales_by_cluster.index, average_sales_by_cluster.values, color=colors)\n",
        "plt.xlabel('Cluster')\n",
        "plt.ylabel('Average Sales')\n",
        "plt.title('Average Sales by Cluster')\n",
        "\n",
        "# Set the x-tick labels as integers\n",
        "plt.xticks(range(1, len(average_sales_by_cluster.index) + 1))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "DXu4PoMRyoK9"
      },
      "execution_count": null,
      "outputs": [],
      "id": "DXu4PoMRyoK9"
    },
    {
      "cell_type": "code",
      "source": [
        "# Sort the data by average sales in descending order\n",
        "average_sales_by_city = average_sales_by_city.sort_values(ascending=True)\n",
        "\n",
        "# Plot the average sales by city horizontally\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.barh(average_sales_by_city.index, average_sales_by_city.values, color=colors)\n",
        "plt.xlabel('Average Sales')\n",
        "plt.ylabel('City')\n",
        "plt.title('Average Sales by City')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "CLcaO1IDyxSb"
      },
      "execution_count": null,
      "outputs": [],
      "id": "CLcaO1IDyxSb"
    },
    {
      "cell_type": "code",
      "source": [
        "# Sort the data by average sales in descending order\n",
        "average_sales_by_state = average_sales_by_state.sort_values(ascending=True)\n",
        "\n",
        "# Plot the average sales by state\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.barh(average_sales_by_state.index, average_sales_by_state.values, color=colors)\n",
        "plt.xlabel('State')\n",
        "plt.ylabel('Average Sales')\n",
        "plt.title('Average Sales by State')\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "gLOR2kBSy3FP"
      },
      "execution_count": null,
      "outputs": [],
      "id": "gLOR2kBSy3FP"
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the average sales by store type\n",
        "plt.bar(average_sales_by_store_type.index, average_sales_by_store_type.values, color=colors)\n",
        "plt.xlabel('Store Type')\n",
        "plt.ylabel('Average Sales')\n",
        "plt.title('Average Sales by Store Type')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "dAR-DIx8y5hY"
      },
      "execution_count": null,
      "outputs": [],
      "id": "dAR-DIx8y5hY"
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Are sales affected by promotions, oil prices and holidays?"
      ],
      "metadata": {
        "id": "I4r726Aiy8v4"
      },
      "id": "I4r726Aiy8v4"
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate correlations between sales and promotions, oil prices, holidays\n",
        "corr_sales_promotions = merged_df['sales'].corr(merged_df['onpromotion'])\n",
        "corr_sales_oil = merged_df['sales'].corr(merged_df['dcoilwtico'])\n",
        "corr_sales_holidays = merged_df['sales'].corr(merged_df['holiday_type'] == 'Holiday')\n",
        "\n",
        "# Print the correlation values\n",
        "print(f\"Correlation between Sales and Promotions: {corr_sales_promotions}\")\n",
        "print(f\"Correlation between Sales and Oil Prices: {corr_sales_oil}\")\n",
        "print(f\"Correlation between Sales and Holidays: {corr_sales_holidays}\")"
      ],
      "metadata": {
        "id": "EFkm9kI3y9d0"
      },
      "execution_count": null,
      "outputs": [],
      "id": "EFkm9kI3y9d0"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Promotions: There is a positive correlation of approximately 0.42 between sales and promotions. This suggests that promotions have a moderate positive impact on sales. When promotions are running, there is an increased likelihood of higher sales.\n",
        "\n",
        "\n",
        "Oil Prices: There is a weak negative correlation of approximately -0.06 between sales and oil prices. This indicates that there is a slight negative relationship between sales and oil prices. However, the correlation is close to zero, suggesting that oil prices have minimal impact on sales.\n",
        "\n",
        "\n",
        "Holidays: There is a very weak negative correlation of approximately -0.04 between sales and holidays. This indicates that there is almost no relationship between sales and holidays. Holidays do not seem to have a significant impact on sales. These insights suggest that promotions have a relatively stronger influence on sales compared to oil prices and holidays. While promotions positively impact sales, oil prices and holidays show minimal or no relationship with sales."
      ],
      "metadata": {
        "id": "4akavSZHzAsJ"
      },
      "id": "4akavSZHzAsJ"
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. What analysis can we get from the date and its extractable features?"
      ],
      "metadata": {
        "id": "JYCR9B28zGTd"
      },
      "id": "JYCR9B28zGTd"
    },
    {
      "cell_type": "code",
      "source": [
        "# Extracting Date Components\n",
        "merged_df_copy['date'] = pd.to_datetime(merged_df_copy['date'])\n",
        "merged_df_copy['year'] = merged_df_copy['date'].dt.year\n",
        "merged_df_copy['month'] = merged_df_copy['date'].dt.month\n",
        "merged_df_copy['day'] = merged_df_copy['date'].dt.day\n",
        "merged_df_copy.head()"
      ],
      "metadata": {
        "id": "FsKjgMREzHAe"
      },
      "execution_count": null,
      "outputs": [],
      "id": "FsKjgMREzHAe"
    },
    {
      "cell_type": "code",
      "source": [
        "# Set distinct colors for each year\n",
        "colors = sns.color_palette(\"husl\", n_colors=len(merged_df_copy['year'].unique()))\n",
        "\n",
        "# Visualize the Monthly Sales Trend\n",
        "monthly_sales = merged_df_copy.groupby(['year', 'month'])['sales'].sum().reset_index()\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.lineplot(data=monthly_sales, x='month', y='sales', hue='year', palette=colors)\n",
        "plt.title('Monthly Sales Trend')\n",
        "plt.xlabel('Month')\n",
        "plt.ylabel('Total Sales')\n",
        "plt.xticks(range(1, 13), labels=['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec'])\n",
        "plt.legend(title='Year')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "tkWw7tJ5zLJw"
      },
      "execution_count": null,
      "outputs": [],
      "id": "tkWw7tJ5zLJw"
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize the Day of the Week Sales Pattern\n",
        "merged_df_copy['day_of_week'] = merged_df_copy['date'].dt.dayofweek\n",
        "day_of_week_sales = merged_df_copy.groupby('day_of_week')['sales'].mean().reset_index()\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.barplot(data=day_of_week_sales, x='day_of_week', y='sales')\n",
        "plt.title('Average Sales by Day of the Week')\n",
        "plt.xlabel('Day of the Week')\n",
        "plt.ylabel('Average Sales')\n",
        "plt.xticks(range(7), labels=['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun'])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "c9wbUTkgzOFj"
      },
      "execution_count": null,
      "outputs": [],
      "id": "c9wbUTkgzOFj"
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize the Yearly Sales Trend\n",
        "yearly_sales = merged_df_copy.groupby('year')['sales'].sum().reset_index()\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.barplot(data=yearly_sales, x='year', y='sales')\n",
        "plt.title('Yearly Sales Trend')\n",
        "plt.xlabel('Year')\n",
        "plt.ylabel('Total Sales')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "B76KEHF6zem7"
      },
      "execution_count": null,
      "outputs": [],
      "id": "B76KEHF6zem7"
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Are there certain product families types that exhibit higher sales performance?"
      ],
      "metadata": {
        "id": "TJjL-zDczh7h"
      },
      "id": "TJjL-zDczh7h"
    },
    {
      "cell_type": "code",
      "source": [
        "# Group the data by product family and calculate the total sales\n",
        "family_sales = merged_df.groupby('family')['sales'].sum().sort_values(ascending=False)\n",
        "\n",
        "# Select the top 10 product families\n",
        "top_10_families = family_sales.head(10)\n",
        "\n",
        "# Plot the relationship between product family and sales for the top 10 families\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.barplot(x=top_10_families.index, y=top_10_families.values, palette='viridis')\n",
        "plt.xlabel('Product Family')\n",
        "plt.ylabel('Total Sales')\n",
        "plt.title('Total Sales by Product Family (Top 10)')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "uuyLclG5ziyK"
      },
      "execution_count": null,
      "outputs": [],
      "id": "uuyLclG5ziyK"
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. How does the sales trend vary across different store numbers?"
      ],
      "metadata": {
        "id": "-OuBMamjzp-Q"
      },
      "id": "-OuBMamjzp-Q"
    },
    {
      "cell_type": "code",
      "source": [
        "store_sales = merged_df.groupby('store_nbr')['sales'].sum()\n",
        "\n",
        "plt.figure(figsize=(14, 6))\n",
        "plt.bar(store_sales.index, store_sales.values, color=cm.viridis(store_sales.values/max(store_sales.values)))\n",
        "plt.xlabel('Store Number')\n",
        "plt.ylabel('Total Sales')\n",
        "plt.title('Sales by Store Number')\n",
        "\n",
        "# Set the X-axis limits and ticks\n",
        "plt.xlim(0, 54)\n",
        "plt.xticks(range(55))\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "0wEKdV4wztk2"
      },
      "execution_count": null,
      "outputs": [],
      "id": "0wEKdV4wztk2"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**FEATURE SCALING**"
      ],
      "metadata": {
        "id": "jVGP2GRKCTbp"
      },
      "id": "jVGP2GRKCTbp"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Standard scaler is used to scale the dataset and make mean 0 and standard deviation 1"
      ],
      "metadata": {
        "id": "MmbabDKiC0M9"
      },
      "id": "MmbabDKiC0M9"
    },
    {
      "cell_type": "code",
      "source": [
        "# Scaling Numeric Variables (Min-Max Scaling)\n",
        "# create an instance of StandardScaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "num_cols = ['sales', 'transactions', 'dcoilwtico']\n",
        "\n",
        "# fit and transform the numerical columns\n",
        "mergeddf_copy[num_cols] = scaler.fit_transform(mergeddf_copy[num_cols])\n",
        "\n",
        "# Display the updated dataframe\n",
        "mergeddf_copy.head()"
      ],
      "metadata": {
        "id": "2YQ_zbcUCSlH"
      },
      "id": "2YQ_zbcUCSlH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Encoding the categorical columns to one hot encoded vector"
      ],
      "metadata": {
        "id": "rUFVeWJoEbJq"
      },
      "id": "rUFVeWJoEbJq"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Define the categorical columns to encode\n",
        "categorical_columns = [\"family\", \"city\", \"holiday_type\"]\n",
        "\n",
        "# Perform one-hot encoding\n",
        "encoder = OneHotEncoder()\n",
        "one_hot_encoded_data = encoder.fit_transform(mergeddf_copy[categorical_columns])\n",
        "\n",
        "# Create column names for the one-hot encoded data\n",
        "column_names = encoder.get_feature_names_out(categorical_columns)\n",
        "\n",
        "# Convert the one-hot encoded data to a DataFrame\n",
        "merged_df_encoded = pd.DataFrame(one_hot_encoded_data.toarray(), columns=column_names)\n",
        "\n",
        "# Concatenate the original dataframe with the one-hot encoded data\n",
        "merged_df_encoded = pd.concat([mergeddf_copy, merged_df_encoded], axis=1)\n",
        "\n",
        "# Drop the original categorical columns\n",
        "merged_df_encoded.drop(categorical_columns, axis=1, inplace=True)\n",
        "\n",
        "# Print the head of the encoded DataFrame\n",
        "merged_df_encoded.head()"
      ],
      "metadata": {
        "id": "JTQYgxoyEojg"
      },
      "id": "JTQYgxoyEojg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TEST DATASET test_df**"
      ],
      "metadata": {
        "id": "pgEw7WfjFoOR"
      },
      "id": "pgEw7WfjFoOR"
    },
    {
      "cell_type": "code",
      "source": [
        "test_df['date'] = pd.to_datetime(test_df['date'])\n",
        "test_df['year'] = test_df['date'].dt.year\n",
        "test_df['month'] = test_df['date'].dt.month\n",
        "test_df['day'] = test_df['date'].dt.day\n",
        "test_df.head()"
      ],
      "metadata": {
        "id": "5BKfY45MF9c2"
      },
      "id": "5BKfY45MF9c2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Drop irrelevant columns"
      ],
      "metadata": {
        "id": "N2T6DrNNGHVY"
      },
      "id": "N2T6DrNNGHVY"
    },
    {
      "cell_type": "code",
      "source": [
        "columns_to_drop = ['date', 'id']\n",
        "test_df = test_df.drop(columns=columns_to_drop)\n",
        "test_df.head()"
      ],
      "metadata": {
        "id": "yXRXiPBqGFvC"
      },
      "id": "yXRXiPBqGFvC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "food_families = ['BEVERAGES', 'BREAD/BAKERY', 'FROZEN FOODS', 'MEATS', 'PREPARED FOODS', 'DELI','PRODUCE', 'DAIRY','POULTRY','EGGS','SEAFOOD']\n",
        "home_families = ['HOME AND KITCHEN I', 'HOME AND KITCHEN II', 'HOME APPLIANCES']\n",
        "clothing_families = ['LINGERIE', 'LADIESWEAR']\n",
        "grocery_families = ['GROCERY I', 'GROCERY II']\n",
        "stationery_families = ['BOOKS', 'MAGAZINES','SCHOOL AND OFFICE SUPPLIES']\n",
        "cleaning_families = ['HOME CARE', 'BABY CARE','PERSONAL CARE']\n",
        "hardware_families = ['PLAYERS AND ELECTRONICS','HARDWARE']\n",
        "\n",
        "test_df['family'] = np.where(test_df['family'].isin(food_families), 'FOODS', test_df['family'])\n",
        "test_df['family'] = np.where(test_df['family'].isin(home_families), 'HOME', test_df['family'])\n",
        "test_df['family'] = np.where(test_df['family'].isin(clothing_families), 'CLOTHING', test_df['family'])\n",
        "test_df['family'] = np.where(test_df['family'].isin(grocery_families), 'GROCERY', test_df['family'])\n",
        "test_df['family'] = np.where(test_df['family'].isin(stationery_families), 'STATIONERY', test_df['family'])\n",
        "test_df['family'] = np.where(test_df['family'].isin(cleaning_families), 'CLEANING', test_df['family'])\n",
        "test_df['family'] = np.where(test_df['family'].isin(hardware_families), 'HARDWARE', test_df['family'])"
      ],
      "metadata": {
        "id": "ibiEYHPGGMFL"
      },
      "id": "ibiEYHPGGMFL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ENCODE CATEGORICAL COLUMNS"
      ],
      "metadata": {
        "id": "e42FRTWzGZWc"
      },
      "id": "e42FRTWzGZWc"
    },
    {
      "cell_type": "code",
      "source": [
        "categorical_columns = [\"family\"]\n",
        "\n",
        "# Create an instance of the OneHotEncoder\n",
        "encoder = OneHotEncoder()\n",
        "\n",
        "# Perform one-hot encoding on the 'test_df' data for the specified categorical columns\n",
        "one_hot_encoded_data = encoder.fit_transform(test_df[categorical_columns])\n",
        "\n",
        "# Get the column names for the one-hot encoded data\n",
        "column_names = encoder.get_feature_names_out(categorical_columns)\n",
        "\n",
        "# Create a DataFrame with the one-hot encoded data and corresponding column names\n",
        "test_df_encoded = pd.DataFrame(one_hot_encoded_data.toarray(), columns=column_names)\n",
        "\n",
        "# Concatenate the original 'test_df' with the one-hot encoded data\n",
        "test_df_encoded = pd.concat([test_df, test_df_encoded], axis=1)\n",
        "\n",
        "# Drop the original categorical columns since they have been encoded\n",
        "test_df_encoded.drop(categorical_columns, axis=1, inplace=True)\n",
        "\n",
        "# Display the updated 'test_df_encoded' DataFrame\n",
        "test_df_encoded.head()"
      ],
      "metadata": {
        "id": "5ykrYR9OGZkU"
      },
      "id": "5ykrYR9OGZkU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**MODELLING**"
      ],
      "metadata": {
        "id": "imSnXiwRGkgo"
      },
      "id": "imSnXiwRGkgo"
    },
    {
      "cell_type": "code",
      "source": [
        "train_set = merged_df_encoded.loc[merged_df_encoded['year'].isin([2013, 2014, 2015, 2016])]\n",
        "eval_set = merged_df_encoded.loc[merged_df_encoded['year'] == 2017]\n"
      ],
      "metadata": {
        "id": "w9mKKObJGpQe"
      },
      "id": "w9mKKObJGpQe",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_set.shape"
      ],
      "metadata": {
        "id": "53jOocYxGy9A"
      },
      "id": "53jOocYxGy9A",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval_set.head()"
      ],
      "metadata": {
        "id": "1-Crw3G_G1_-"
      },
      "id": "1-Crw3G_G1_-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Separate the target variable and features for training and testing\n",
        "X_train = train_set.drop('sales', axis=1)\n",
        "y_train = train_set['sales']\n",
        "\n",
        "\n",
        "X_eval = eval_set.drop('sales', axis=1)\n",
        "y_eval = eval_set['sales']"
      ],
      "metadata": {
        "id": "tpd7aZUcHs1I"
      },
      "id": "tpd7aZUcHs1I",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results_df = pd.DataFrame(columns=['Model', 'RMSLE', 'RMSE', 'MSE', 'MAE'])"
      ],
      "metadata": {
        "id": "tYj5aEolH0O-"
      },
      "id": "tYj5aEolH0O-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**MODEL TRAINING**"
      ],
      "metadata": {
        "id": "fU8iIxgnH1jq"
      },
      "id": "fU8iIxgnH1jq"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**LINEAR REGRESSION**"
      ],
      "metadata": {
        "id": "8yQCSs6OI1bd"
      },
      "id": "8yQCSs6OI1bd"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Linear Regression is a predictive modeling technique used to find a linear relationship between a dependent variable (the one you want to predict) and one or more independent variables (the features you use for prediction)."
      ],
      "metadata": {
        "id": "Ywx488FgI3M8"
      },
      "id": "Ywx488FgI3M8"
    },
    {
      "cell_type": "code",
      "source": [
        "# Linear Regression Model\n",
        "lr_model = LinearRegression()\n",
        "lr_model.fit(X_train, y_train)\n",
        "lr_predictions = lr_model.predict(X_eval)\n",
        "\n",
        "# Calculate metrics\n",
        "lr_mse = mean_squared_error(y_eval, lr_predictions)\n",
        "lr_mae = mean_absolute_error(y_eval, lr_predictions)\n",
        "\n",
        "# Apply the absolute value function to both y_eval and lr_predictions\n",
        "y_eval_abs = abs(y_eval)\n",
        "lr_predictions_abs = abs(lr_predictions)\n",
        "\n",
        "# Calculate the Root Mean Squared Logarithmic Error (RMSLE)\n",
        "lr_rmsle = np.sqrt(mean_squared_log_error(y_eval_abs, lr_predictions_abs))\n",
        "\n",
        "# Create a DataFrame to store results for Linear Regression\n",
        "results_lr = pd.DataFrame({'Model': ['Linear Regression'],\n",
        "                            'RMSLE': [lr_rmsle],\n",
        "                            'RMSE': [np.sqrt(lr_mse)],\n",
        "                            'MSE': [lr_mse],\n",
        "                            'MAE': [lr_mae]}).round(2)\n",
        "\n",
        "# Print the results_lr dataframe\n",
        "results_lr"
      ],
      "metadata": {
        "id": "0g-5SSyRH62m"
      },
      "id": "0g-5SSyRH62m",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**RANDOM FOREST**"
      ],
      "metadata": {
        "id": "xRH0M2b7KCTD"
      },
      "id": "xRH0M2b7KCTD"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Random Forest is an ensemble learning method. Instead of relying on a single complex model, it builds a large number of individual Decision Trees and then combines their predictions to make a final, more accurate prediction."
      ],
      "metadata": {
        "id": "cLlpH7r1JgOX"
      },
      "id": "cLlpH7r1JgOX"
    },
    {
      "cell_type": "code",
      "source": [
        "# Random Forest Regression Model\n",
        "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "rf_model.fit(X_train, y_train)\n",
        "rf_predictions = rf_model.predict(X_eval)\n",
        "\n",
        "# Calculate metrics\n",
        "rf_mse = mean_squared_error(y_eval, rf_predictions)\n",
        "rf_mae = mean_absolute_error(y_eval, rf_predictions)\n",
        "\n",
        "# Apply the absolute value function to both y_eval and rf_predictions\n",
        "y_eval_abs = abs(y_eval)\n",
        "rf_predictions_abs = abs(rf_predictions)\n",
        "\n",
        "# Calculate the Root Mean Squared Logarithmic Error (RMSLE)\n",
        "rf_rmsle = np.sqrt(mean_squared_log_error(y_eval_abs, rf_predictions_abs))\n",
        "\n",
        "# Create a DataFrame to store results for Random Forest\n",
        "results_rf = pd.DataFrame({'Model': ['Random Forest'],\n",
        "                            'RMSLE': [rf_rmsle],\n",
        "                            'RMSE': [np.sqrt(rf_mse)],\n",
        "                            'MSE': [rf_mse],\n",
        "                            'MAE': [rf_mae]}).round(2)\n",
        "\n",
        "# Print the results_rf dataframe\n",
        "results_rf"
      ],
      "metadata": {
        "id": "0W16IgcnKB5A"
      },
      "id": "0W16IgcnKB5A",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Gradient Boosting**"
      ],
      "metadata": {
        "id": "btt-04M9Luw_"
      },
      "id": "btt-04M9Luw_"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gradient Boosting is an ensemble learning method that builds models sequentially.\n",
        "\n",
        "Instead of growing many trees independently (like Random Forest), Gradient Boosting builds one tree at a time.\n",
        "\n",
        "Each new tree tries to fix the errors made by the previous trees.\n",
        "\n",
        "It minimizes a loss function (like MSE) using gradient descent, hence the name."
      ],
      "metadata": {
        "id": "wk8snyuQLZKf"
      },
      "id": "wk8snyuQLZKf"
    },
    {
      "cell_type": "code",
      "source": [
        "# Gradient Boosting Regression Model\n",
        "gb_model = GradientBoostingRegressor(n_estimators=100, random_state=42)\n",
        "gb_model.fit(X_train, y_train)\n",
        "gb_predictions = gb_model.predict(X_eval)\n",
        "\n",
        "# Calculate metrics\n",
        "gb_mse = mean_squared_error(y_eval, gb_predictions)\n",
        "gb_mae = mean_absolute_error(y_eval, gb_predictions)\n",
        "\n",
        "# Apply the absolute value function to both y_eval and gb_predictions\n",
        "y_eval_abs = abs(y_eval)\n",
        "gb_predictions_abs = abs(gb_predictions)\n",
        "\n",
        "# Calculate the Root Mean Squared Logarithmic Error (RMSLE)\n",
        "gb_rmsle = np.sqrt(mean_squared_log_error(y_eval_abs, gb_predictions_abs))\n",
        "\n",
        "# Create a DataFrame to store results for Gradient Boosting\n",
        "results_gb = pd.DataFrame({'Model': ['Gradient Boosting'],\n",
        "                            'RMSLE': [gb_rmsle],\n",
        "                            'RMSE': [np.sqrt(gb_mse)],\n",
        "                            'MSE': [gb_mse],\n",
        "                            'MAE': [gb_mae]}).round(2)\n",
        "\n",
        "# Print the results_gb dataframe\n",
        "results_gb"
      ],
      "metadata": {
        "id": "zBAQ5iG6Ly9t"
      },
      "id": "zBAQ5iG6Ly9t",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ARIMA**\n",
        "\n",
        "\n",
        "\n",
        "ARIMA stands for AutoRegressive Integrated Moving Average.\n",
        "\n",
        "It’s a time series forecasting model that combines three components:\n",
        "\n",
        "AR (AutoRegressive, p): relationship between current value and its past values.\n",
        "\n",
        "I (Integrated, d): differencing to make the series stationary.\n",
        "\n",
        "MA (Moving Average, q): relationship between current value and past forecast errors.\n",
        "\n",
        "Best for univariate time series forecasting."
      ],
      "metadata": {
        "id": "cg4NRL_OLyld"
      },
      "id": "cg4NRL_OLyld"
    },
    {
      "cell_type": "code",
      "source": [
        "# d and q are equal to zero as data is already stationary\n",
        "p = 1\n",
        "d = 0\n",
        "q = 0\n",
        "\n",
        "# Create an instance of the ARIMA model\n",
        "arima_model = ARIMA(y_train, order=(p, d, q))\n",
        "\n",
        "# Fit the model to the training data\n",
        "arima_model_fit = arima_model.fit()\n",
        "\n",
        "# Make predictions on the evaluation data\n",
        "arima_predictions = arima_model_fit.predict(start=len(y_train), end=len(y_train) + len(X_eval) - 1)\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "arima_mse = mean_squared_error(y_eval, arima_predictions)\n",
        "arima_rmse = np.sqrt(arima_mse)\n",
        "\n",
        "# Apply the absolute value function to y_eval to remove negative signs\n",
        "y_eval_abs = abs(y_eval)\n",
        "arima_predictions_abs = abs(arima_predictions)\n",
        "\n",
        "# Calculate the Mean Absolute Error (MAE)\n",
        "arima_mae = mean_absolute_error(y_eval, arima_predictions)\n",
        "\n",
        "# Calculate the Root Mean Squared Logarithmic Error (RMSLE)\n",
        "arima_rmsle = np.sqrt(mean_squared_log_error(y_eval_abs, arima_predictions_abs))\n",
        "\n",
        "# Create a DataFrame to store results for ARIMA\n",
        "results_arima = pd.DataFrame({'Model': ['ARIMA'],\n",
        "                            'RMSLE': [arima_rmsle],\n",
        "                            'RMSE': [np.sqrt(arima_mse)],\n",
        "                            'MSE': [arima_mse],\n",
        "                            'MAE': [arima_mae]}).round(2)\n",
        "\n",
        "# Print the results_arima dataframe\n",
        "results_arima"
      ],
      "metadata": {
        "id": "S1JHNg6YQWX6"
      },
      "id": "S1JHNg6YQWX6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**SARIMA**\n",
        "\n",
        "\n",
        "SARIMA stands for Seasonal AutoRegressive Integrated Moving Average.\n",
        "\n",
        "It is an extension of ARIMA that can handle seasonality in time series.\n",
        "\n",
        "Components:\n",
        "\n",
        "(p, d, q): same as ARIMA → autoregression, differencing, moving average.\n",
        "\n",
        "(P, D, Q, s): seasonal part → captures repeating seasonal patterns.\n",
        "\n",
        "P: seasonal autoregressive order.\n",
        "\n",
        "D: seasonal differencing order.\n",
        "\n",
        "Q: seasonal moving average order."
      ],
      "metadata": {
        "id": "fKrvQD5pQ7k4"
      },
      "id": "fKrvQD5pQ7k4"
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the order and seasonal order parameters\n",
        "# Seasonal autoregressive order\n",
        "P = 0\n",
        "# Seasonal differencing order\n",
        "D = 0\n",
        "# Seasonal moving average order\n",
        "Q = 0\n",
        "# Number of time steps in each season (chosen based on the number of months each year)\n",
        "s = 12\n",
        "\n",
        "# Create an instance of the SARIMA model\n",
        "sarima_model = SARIMAX(endog=y_train, exog=X_train, order=(p, d, q), seasonal_order=(P, D, Q, s))\n",
        "\n",
        "# Fit the model to the training data\n",
        "sarima_fit = sarima_model.fit()\n",
        "\n",
        "# Make predictions on the evaluation data\n",
        "sarima_predictions = sarima_fit.forecast(steps=len(y_eval), exog=X_eval)\n",
        "\n",
        "# Calculate metrics\n",
        "sarima_mse = mean_squared_error(y_eval, sarima_predictions)\n",
        "sarima_rmse = np.sqrt(sarima_mse)\n",
        "sarima_mae = mean_absolute_error(y_eval, sarima_predictions)\n",
        "sarima_rmsle = np.sqrt(mean_squared_error(np.log1p(y_eval), np.log1p(sarima_predictions)))\n",
        "\n",
        "# Create a DataFrame to store results for SARIMA\n",
        "results_sarima = pd.DataFrame({'Model': ['SARIMA'],\n",
        "                                'RMSLE': [sarima_rmsle],\n",
        "                                'RMSE': [sarima_rmse],\n",
        "                                'MSE': [sarima_mse],\n",
        "                                'MAE': [sarima_mae]}).round(2)\n",
        "\n",
        "# Print the results_sarima dataframe\n",
        "results_sarima"
      ],
      "metadata": {
        "id": "wHHd4Zg7RCht"
      },
      "id": "wHHd4Zg7RCht",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**XGBOOST**"
      ],
      "metadata": {
        "id": "pzuXjfkT8SCu"
      },
      "id": "pzuXjfkT8SCu"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Create the model\n",
        "xg_reg = xgb.XGBRegressor(objective ='reg:squarederror', n_estimators =100, learning_rate=0.1)\n",
        "\n",
        "# Fit the model\n",
        "xg_reg.fit(X_train, y_train)\n",
        "\n",
        "# Predict on training set (or on test set if available)\n",
        "y_pred = xg_reg.predict(X_train)\n",
        "\n",
        "# Evaluate\n",
        "xgboost_mse = mean_squared_error(y_eval, sarima_predictions)\n",
        "xgboost_rmse = np.sqrt(sarima_mse)\n",
        "xgboost_mae = mean_absolute_error(y_eval, sarima_predictions)\n",
        "xgboost_rmsle = np.sqrt(mean_squared_error(np.log1p(y_eval), np.log1p(sarima_predictions)))\n",
        "\n",
        "results_xgboost = pd.DataFrame({'Model': ['XGBOOST'],\n",
        "                                'RMSLE': [xgboost_rmsle],\n",
        "                                'RMSE': [xgboost_rmse],\n",
        "                                'MSE': [xgboost_mse],\n",
        "                                'MAE': [xgboost_mae]}).round(2)\n",
        "results_xgboost\n"
      ],
      "metadata": {
        "id": "2KDhY5v-8XVT"
      },
      "id": "2KDhY5v-8XVT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Append all results to the results dataframe\n",
        "results_df = pd.concat([results_df, results_lr], ignore_index=True)\n",
        "results_df = pd.concat([results_df, results_rf], ignore_index=True)\n",
        "results_df = pd.concat([results_df, results_gb], ignore_index=True)\n",
        "results_df = pd.concat([results_df, results_arima], ignore_index=True)\n",
        "results_df = pd.concat([results_df, results_sarima], ignore_index=True)\n",
        "results_df = pd.concat([results_df, results_xgboost],ignore_index=True)\n",
        "\n",
        "# Sort the results_df based on RMSE in ascending order\n",
        "results_df = results_df.sort_values(by='RMSLE', ascending=True)\n",
        "\n",
        "# Reset the index of the DataFrame\n",
        "results_df = results_df.reset_index(drop=True)\n",
        "\n",
        "# Print the Final Results dataframe\n",
        "results_df.drop_duplicates()"
      ],
      "metadata": {
        "id": "cqMoheM9TOom"
      },
      "id": "cqMoheM9TOom",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The RMSLE is a metric used when the target variable has a wide range of values. A lower RMSLE indicates a better model fit.\n",
        "The RMSE measures the average magnitude of the errors between predicted and actual values. A lower RMSE indicates better model performance.\n",
        "The MSE is the average of the squared errors and provides a measure of the model's overall accuracy. A lower MSE indicates better performance.\n",
        "The MAE measures the average magnitude of the errors without considering their direction. A lower MAE indicates better model accuracy.\n",
        "The RMSLE and RMSE represent the model's prediction error, with lower values indicating better performance. The MSE provides a measure of the squared error between predicted and actual values, while the MAE measures the average absolute difference between predictions and actuals.\n",
        "\n",
        "These results indicate the performance of each model in terms of different metrics. Lower values of RMSLE, RMSE, MSE, and MAE indicate better model performance. Based on these results, the Random Forest and Gradient Boosting models appear to perform better than the Linear Regression, ARIMA, and SARIMA models in terms of the provided metrics.\n",
        "\n",
        "\n",
        "Best performing model overall:  Random Forest has the lowest RMSLE (0.23) and lowest MAE (0.25), indicating better accuracy and robustness to outliers.\n",
        "\n",
        "Gradient Boosting shows slightly higher RMSLE (0.24) but lower RMSE (0.67) and MSE (0.45), meaning it's good in terms of minimizing large errors but slightly worse than Random Forest in relative error.\n",
        "\n",
        "Linear Regression and SARIMA perform similarly, with higher errors compared to ensemble models.\n",
        "\n",
        "ARIMA performs the worst across all metrics, indicating it's less suitable for this dataset or problem."
      ],
      "metadata": {
        "id": "rdl-7_HiVTJI"
      },
      "id": "rdl-7_HiVTJI"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**HYPERPARAMETER TUNING**"
      ],
      "metadata": {
        "id": "ppaDkIhtVoxk"
      },
      "id": "ppaDkIhtVoxk"
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the parameter grid for tuning the random forest model\n",
        "param_grid_rf = {\n",
        "    'n_estimators': [100, 200, 300, 400, 500],\n",
        "    'max_depth': [5, 10, 15, None],\n",
        "    'min_samples_split': [2, 4, 6],\n",
        "    'min_samples_leaf': [1, 2, 3],\n",
        "    'max_features': ['sqrt', 'log2', 0.5]\n",
        "}\n",
        "\n",
        "# Create Random Forest model\n",
        "rf_model = RandomForestRegressor(random_state=42)\n",
        "\n",
        "# Initialize RandomizedSearchCV\n",
        "random_search_rf = RandomizedSearchCV(rf_model, param_distributions=param_grid_rf,\n",
        "                                      n_iter=10, scoring='neg_mean_squared_error', cv=5,\n",
        "                                      n_jobs=-1, random_state=42)\n",
        "\n",
        "# Fit RandomizedSearchCV to the data\n",
        "random_search_rf.fit(X_train, y_train)\n",
        "\n",
        "# Get the best model and its hyperparameters\n",
        "best_rf_model = random_search_rf.best_estimator_\n",
        "best_rf_params = random_search_rf.best_params_\n",
        "\n",
        "# Make predictions using the best model\n",
        "best_rf_predictions = best_rf_model.predict(X_eval)\n",
        "\n",
        "# Calculate metrics for the best model\n",
        "best_rf_mse = mean_squared_error(y_eval, best_rf_predictions)\n",
        "best_rf_rmse = np.sqrt(best_rf_mse)\n",
        "best_rf_mae = mean_absolute_error(y_eval, best_rf_predictions)\n",
        "\n",
        "# Apply absolute value to both predicted and target values\n",
        "abs_best_rf_predictions = np.abs(best_rf_predictions)\n",
        "abs_y_eval = np.abs(y_eval)\n",
        "\n",
        "# Calculate RMSLE using the absolute values\n",
        "best_rf_rmsle = np.sqrt(mean_squared_log_error(abs_y_eval, abs_best_rf_predictions))\n",
        "\n",
        "# Create a DataFrame to store results for the best Random Forest model\n",
        "best_results_rf = pd.DataFrame({'Model': ['Best Random Forest'],\n",
        "                                'RMSLE': [best_rf_rmsle],\n",
        "                                'RMSE': [best_rf_rmse],\n",
        "                                'MSE': [best_rf_mse],\n",
        "                                'MAE': [best_rf_mae]}).round(2)\n",
        "\n",
        "# Print the best_results_rf dataframe\n",
        "best_results_rf"
      ],
      "metadata": {
        "id": "qt2mqdq4V3Z0"
      },
      "id": "qt2mqdq4V3Z0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the best parameters\n",
        "print(\"Best Parameters for Random Forest Model:\")\n",
        "print(best_rf_params)"
      ],
      "metadata": {
        "id": "NQAoN3GuWC9a"
      },
      "id": "NQAoN3GuWC9a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Random Forest model was tuned using a randomized search to find the best combination of hyperparameters that optimize its performance. The search yielded the following set of optimal hyperparameters:\n",
        "\n",
        "Number of Estimators (Trees): 500\n",
        "Minimum Samples Split: 2\n",
        "Minimum Samples Leaf: 1\n",
        "Maximum Features: 0.5\n",
        "Maximum Depth: 10\n",
        "\n",
        "\n",
        "\n",
        "These hyperparameters represent the configuration that resulted in the best performance for the Random Forest model on the evaluation data. The hyperparameters were selected based on their impact on the model's ability to minimize the Mean Squared Error (MSE), which is a common measure of predictive accuracy. By fine-tuning these hyperparameters, the model's ability to generalize to unseen data has been significantly improved.\n",
        "\n",
        "After applying these optimal hyperparameters, the Random Forest model's performance on the evaluation data is as follows:\n",
        "\n",
        "Root Mean Squared Logarithmic Error (RMSLE): 0.21 After tuning the Random Forest model, the RMSLE improved from 0.22 to 0.21, suggesting that the model's ability to handle the variation in target values has improved.\n",
        "\n",
        "Root Mean Squared Error (RMSE): 0.59 After tuning, the RMSE decreased from 0.71 to 0.59, indicating that the model's predictions are more accurate on average.\n",
        "\n",
        "Mean Squared Error (MSE): 0.35 After tuning, the MSE decreased from 0.51 to 0.35, indicating that the model's predictions are closer to the actual values on average.\n",
        "\n",
        "Mean Absolute Error (MAE): 0.23 After tuning, the MAE decreased from 0.24 to 0.23, suggesting that the model's predictions are closer to the true values on average.\n",
        "\n",
        "The hyperparameter tuning process for the Random Forest model led to improvements in all evaluation metrics, indicating that the model's performance has been enhanced. This suggests that the tuned Random Forest model is a better fit for the data and is capable of making more accurate predictions on the target variable These performance metrics provide insights into the model's accuracy and how well it predicts sales values.\n",
        "\n",
        "In summary, the Random Forest model, with its optimized hyperparameters, demonstrates improved predictive performance compared to its initial configuration. It effectively captures the relationships between features and sales, resulting in more accurate predictions on the evaluation dataset."
      ],
      "metadata": {
        "id": "YHOHj27JWPC7"
      },
      "id": "YHOHj27JWPC7"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**SAVING THE BEST MODEL**"
      ],
      "metadata": {
        "id": "cv68Tua8Wcac"
      },
      "id": "cv68Tua8Wcac"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "key_components = {\n",
        "    'model': best_rf_model,\n",
        "    'best_params': best_rf_params,\n",
        "    'best_score': best_rf_rmsle\n",
        "}\n",
        "\n",
        "\n",
        "with open('best_rf_model_components.pkl', 'wb') as file:\n",
        "    pickle.dump(key_components, file)"
      ],
      "metadata": {
        "id": "Cn94SAMQWhHu"
      },
      "id": "Cn94SAMQWhHu",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "required_libs": [],
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}